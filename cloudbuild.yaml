substitutions:
  _REGION: us-central1
  _REPO: cloud-run-repo

options:
  logging: CLOUD_LOGGING_ONLY

steps:

# 1. Detect Environment by Branch
- name: gcr.io/cloud-builders/gcloud
  id: detect-env
  entrypoint: bash
  args:
    - -c
    - |
      if [[ "$BRANCH_NAME" == "main" ]]; then
        echo "prod" > /workspace/env
      elif [[ "$BRANCH_NAME" == "staging" ]]; then
        echo "staging" > /workspace/env
      elif [[ "$BRANCH_NAME" == "dev-ci-cd" ]]; then
        echo "dev-ci-cd" > /workspace/env
      else
        echo "Branch $BRANCH_NAME does not correspond to a deployment environment."
        exit 1
      fi

# 2. Backend â€“ Python
- name: gcr.io/cloud-builders/docker
  id: build-backend
  entrypoint: bash
  args:
    - -c
    - |
      ENV=$(cat /workspace/env)
      docker build \
        -t ${_REGION}-docker.pkg.dev/$PROJECT_ID/${_REPO}/backend-$${ENV}:$SHORT_SHA \
        ./backend

- name: gcr.io/cloud-builders/docker
  id: push-backend
  entrypoint: bash
  args:
    - -c
    - |
      ENV=$(cat /workspace/env)
      docker push ${_REGION}-docker.pkg.dev/$PROJECT_ID/${_REPO}/backend-$${ENV}:$SHORT_SHA

- name: gcr.io/google.com/cloudsdktool/cloud-sdk
  id: deploy-backend
  entrypoint: bash
  args:
    - -c
    - |
      ENV=$(cat /workspace/env)
      
      python3 - <<'PY' > /workspace/env_vars.json
      import json
      d = {
        "OPENAI_API_KEY": "${_OPENAI_API_KEY}","DIFFBOT_API_KEY": "${_DIFFBOT_API_KEY}","BUCKET_UPLOAD_FILE": "${_BUCKET_UPLOAD_FILE}","BUCKET_FAILED_FILE": "${_BUCKET_FAILED_FILE}",
        "PROJECT_ID": "${_PROJECT_ID}","GCS_FILE_CACHE": "${_GCS_FILE_CACHE}","TRACK_TOKEN_USAGE": "${_TRACK_TOKEN_USAGE}","TOKEN_TRACKER_DB_URI": "${_TOKEN_TRACKER_DB_URI}",
        "TOKEN_TRACKER_DB_USERNAME": "${_TOKEN_TRACKER_DB_USERNAME}","TOKEN_TRACKER_DB_PASSWORD": "${_TOKEN_TRACKER_DB_PASSWORD}","TOKEN_TRACKER_DB_DATABASE": "${_TOKEN_TRACKER_DB_DATABASE}",
        "DEFAULT_DIFFBOT_CHAT_MODEL": "${_DEFAULT_DIFFBOT_CHAT_MODEL}","RAGAS_EMBEDDING_MODEL": "${_RAGAS_EMBEDDING_MODEL}","YOUTUBE_TRANSCRIPT_PROXY": "${_YOUTUBE_TRANSCRIPT_PROXY}",
        "BEDROCK_EMBEDDING_MODEL": "${_BEDROCK_EMBEDDING_MODEL}","LLM_MODEL_CONFIG_OPENAI_GPT_5_1": "${_LLM_MODEL_CONFIG_OPENAI_GPT_5_1}","LLM_MODEL_CONFIG_OPENAI_GPT_5_MINI": "${_LLM_MODEL_CONFIG_OPENAI_GPT_5_MINI}",
        "LLM_MODEL_CONFIG_GEMINI_2_5_FLASH": "${_LLM_MODEL_CONFIG_GEMINI_2_5_FLASH}","LLM_MODEL_CONFIG_GEMINI_2_5_PRO": "${_LLM_MODEL_CONFIG_GEMINI_2_5_PRO}","LLM_MODEL_CONFIG_DIFFBOT": "${_LLM_MODEL_CONFIG_DIFFBOT}",
        "LLM_MODEL_CONFIG_GROQ_LLAMA3_1_8B": "${_LLM_MODEL_CONFIG_GROQ_LLAMA3_1_8B}","LLM_MODEL_CONFIG_ANTHROPIC_CLAUDE_4_5_SONNET": "${_LLM_MODEL_CONFIG_ANTHROPIC_CLAUDE_4_5_SONNET}",
        "LLM_MODEL_CONFIG_ANTHROPIC_CLAUDE_4_5_HAIKU": "${_LLM_MODEL_CONFIG_ANTHROPIC_CLAUDE_4_5_HAIKU}","LLM_MODEL_CONFIG_FIREWORKS_QWEN3_30B": "${_LLM_MODEL_CONFIG_FIREWORKS_QWEN3_30B}",
        "LLM_MODEL_CONFIG_FIREWORKS_GPT_OSS": "${_LLM_MODEL_CONFIG_FIREWORKS_GPT_OSS}","LLM_MODEL_CONFIG_FIREWORKS_DEEPSEEK_V3": "${_LLM_MODEL_CONFIG_FIREWORKS_DEEPSEEK_V3}",
        "LLM_MODEL_CONFIG_BEDROCK_NOVA_MICRO_V1": "${_LLM_MODEL_CONFIG_BEDROCK_NOVA_MICRO_V1}","LLM_MODEL_CONFIG_BEDROCK_NOVA_LITE_V1": "${_LLM_MODEL_CONFIG_BEDROCK_NOVA_LITE_V1}",
        "LLM_MODEL_CONFIG_BEDROCK_NOVA_PRO_V1": "${_LLM_MODEL_CONFIG_BEDROCK_NOVA_PRO_V1}","LLM_MODEL_CONFIG_OLLAMA_LLAMA3": "${_LLM_MODEL_CONFIG_OLLAMA_LLAMA3}","LOG_LEVEL": "${_LOG_LEVEL}"
      }
      
      json.dump(d, open("/workspace/env_vars.json","w"), indent=2)
      PY

      echo "Deploying to Cloud Run service: backend-$${ENV}-new"
      cat /workspace/env_vars.json

      gcloud run deploy backend-$${ENV}-new \
        --image ${_REGION}-docker.pkg.dev/$PROJECT_ID/${_REPO}/backend-$${ENV}:$SHORT_SHA \
        --region ${_REGION} \
        --platform managed \
        --allow-unauthenticated \
        --env-vars-file=/workspace/env_vars.json
        --timeout=1200s

- name: gcr.io/google.com/cloudsdktool/cloud-sdk
  id: deploy-backend-processing
  entrypoint: bash
  args:
    - -c
    - |
      ENV=$(cat /workspace/env)

      # Build JSON env file that preserves commas/quotes/newlines.
      python3 - <<'PY' > /workspace/env_vars.json
      import json
      d = {
        "OPENAI_API_KEY": "${_OPENAI_API_KEY}","DIFFBOT_API_KEY": "${_DIFFBOT_API_KEY}","BUCKET_UPLOAD_FILE": "${_BUCKET_UPLOAD_FILE}","BUCKET_FAILED_FILE": "${_BUCKET_FAILED_FILE}",
        "PROJECT_ID": "${_PROJECT_ID}","GCS_FILE_CACHE": "${_GCS_FILE_CACHE}","TRACK_TOKEN_USAGE": "${_TRACK_TOKEN_USAGE}","TOKEN_TRACKER_DB_URI": "${_TOKEN_TRACKER_DB_URI}",
        "TOKEN_TRACKER_DB_USERNAME": "${_TOKEN_TRACKER_DB_USERNAME}","TOKEN_TRACKER_DB_PASSWORD": "${_TOKEN_TRACKER_DB_PASSWORD}","TOKEN_TRACKER_DB_DATABASE": "${_TOKEN_TRACKER_DB_DATABASE}",
        "DEFAULT_DIFFBOT_CHAT_MODEL": "${_DEFAULT_DIFFBOT_CHAT_MODEL}","RAGAS_EMBEDDING_MODEL": "${_RAGAS_EMBEDDING_MODEL}","YOUTUBE_TRANSCRIPT_PROXY": "${_YOUTUBE_TRANSCRIPT_PROXY}",
        "BEDROCK_EMBEDDING_MODEL": "${_BEDROCK_EMBEDDING_MODEL}","LLM_MODEL_CONFIG_OPENAI_GPT_5_1": "${_LLM_MODEL_CONFIG_OPENAI_GPT_5_1}","LLM_MODEL_CONFIG_OPENAI_GPT_5_MINI": "${_LLM_MODEL_CONFIG_OPENAI_GPT_5_MINI}",
        "LLM_MODEL_CONFIG_GEMINI_2_5_FLASH": "${_LLM_MODEL_CONFIG_GEMINI_2_5_FLASH}","LLM_MODEL_CONFIG_GEMINI_2_5_PRO": "${_LLM_MODEL_CONFIG_GEMINI_2_5_PRO}","LLM_MODEL_CONFIG_DIFFBOT": "${_LLM_MODEL_CONFIG_DIFFBOT}",
        "LLM_MODEL_CONFIG_GROQ_LLAMA3_1_8B": "${_LLM_MODEL_CONFIG_GROQ_LLAMA3_1_8B}","LLM_MODEL_CONFIG_ANTHROPIC_CLAUDE_4_5_SONNET": "${_LLM_MODEL_CONFIG_ANTHROPIC_CLAUDE_4_5_SONNET}",
        "LLM_MODEL_CONFIG_ANTHROPIC_CLAUDE_4_5_HAIKU": "${_LLM_MODEL_CONFIG_ANTHROPIC_CLAUDE_4_5_HAIKU}","LLM_MODEL_CONFIG_FIREWORKS_QWEN3_30B": "${_LLM_MODEL_CONFIG_FIREWORKS_QWEN3_30B}",
        "LLM_MODEL_CONFIG_FIREWORKS_GPT_OSS": "${_LLM_MODEL_CONFIG_FIREWORKS_GPT_OSS}","LLM_MODEL_CONFIG_FIREWORKS_DEEPSEEK_V3": "${_LLM_MODEL_CONFIG_FIREWORKS_DEEPSEEK_V3}",
        "LLM_MODEL_CONFIG_BEDROCK_NOVA_MICRO_V1": "${_LLM_MODEL_CONFIG_BEDROCK_NOVA_MICRO_V1}","LLM_MODEL_CONFIG_BEDROCK_NOVA_LITE_V1": "${_LLM_MODEL_CONFIG_BEDROCK_NOVA_LITE_V1}",
        "LLM_MODEL_CONFIG_BEDROCK_NOVA_PRO_V1": "${_LLM_MODEL_CONFIG_BEDROCK_NOVA_PRO_V1}","LLM_MODEL_CONFIG_OLLAMA_LLAMA3": "${_LLM_MODEL_CONFIG_OLLAMA_LLAMA3}","LOG_LEVEL": "${_LOG_LEVEL}"
      }
      json.dump(d, open("/workspace/env_vars.json","w"), indent=2)
      PY
      
      gcloud run deploy backend-$${ENV}-processing-new \
        --image ${_REGION}-docker.pkg.dev/$PROJECT_ID/${_REPO}/backend-$${ENV}:$SHORT_SHA \
        --region ${_REGION} \
        --platform managed \
        --allow-unauthenticated \
        --env-vars-file=/workspace/env_vars.json \
        --timeout=1200s

# - name: gcr.io/cloud-builders/docker
#   id: build-frontend
#   entrypoint: bash
#   args:
#     - -c
#     - |
#       ENV=$(cat /workspace/env)
#       docker build \
#         --build-arg NEXT_PUBLIC_ENV=$ENV \
#         --build-arg NEXT_PUBLIC_API_URL=https://backend-$ENV-$PROJECT_ID.run.app \
#         -t $_REGION-docker.pkg.dev/$PROJECT_ID/$_REPO/frontend-$ENV:$SHORT_SHA \
#         ./frontend

# - name: gcr.io/cloud-builders/docker
#   id: push-frontend
#   entrypoint: bash
#   args:
#     - -c
#     - |
#       ENV=$(cat /workspace/env)
#       docker push $_REGION-docker.pkg.dev/$PROJECT_ID/$_REPO/frontend-$ENV:$SHORT_SHA

# - name: gcr.io/google.com/cloudsdktool/cloud-sdk
#   id: deploy-frontend
#   entrypoint: bash
#   args:
#     - -c
#     - |
#       ENV=$(cat /workspace/env)
#       gcloud run deploy frontend-$ENV \
#         --image $_REGION-docker.pkg.dev/$PROJECT_ID/$_REPO/frontend-$ENV:$SHORT_SHA \
#         --region $_REGION \
#         --platform managed \
#         --allow-unauthenticated \
#         --set-env-vars \
#           ENV=$ENV,\
#           NEXT_PUBLIC_API_URL=https://backend-$ENV-$PROJECT_ID.run.app