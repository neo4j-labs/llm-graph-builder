{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging face Babelscape/rebel-large model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install datasets\n",
    "!pip install langchain\n",
    "!pip install transformers\n",
    "!pip install neo4j\n",
    "!pip install -U huggingface_hub\n",
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import hashlib\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from datetime import datetime\n",
    "\n",
    "loader = PyPDFLoader('../data/Football_news.pdf')\n",
    "start_time = datetime.now()\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "contents=''\n",
    "for i in range(0,len(pages)):\n",
    "    contents=' '.join([contents,pages[i].page_content.replace('\\n',' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating chunks using langchain RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(contents)\n",
    "#chunk_documents = text_splitter.create_documents([contents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to add id to the chunks\n",
    "# m = hashlib.md5()\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# data = []\n",
    "# m = hashlib.md5()\n",
    "# m.update(contents.encode('utf-8'))\n",
    "# uid = m.hexdigest()[:12]\n",
    "\n",
    "# for i, chunk in enumerate(chunks):\n",
    "#         data.append({\n",
    "#             'id': f'{uid}-{i}',\n",
    "#             'text': chunk\n",
    "#         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Extract triplets from text\n",
    "     \n",
    "     input : string text\n",
    "     ouput: list of tuple (triplets in the form of [source, relation, target])\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")\n",
    "\n",
    "gen_kwargs = {\n",
    "    \"max_length\": 256,\n",
    "    \"length_penalty\": 0,\n",
    "    \"num_beams\": 3,\n",
    "    \"num_return_sequences\": 1,\n",
    "}\n",
    "\n",
    "triples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_triples(texts):\n",
    "\n",
    "  model_inputs = tokenizer(texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "  generated_tokens = model.generate(\n",
    "      model_inputs[\"input_ids\"].to(model.device),\n",
    "      attention_mask=model_inputs[\"attention_mask\"].to(model.device),\n",
    "      **gen_kwargs\n",
    "  )\n",
    "  decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "  for idx, sentence in enumerate(decoded_preds):\n",
    "      et = extract_triplets(sentence)\n",
    "      for t in et:\n",
    "        triples.append((t['head'], t['type'], t['tail']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fac788e7c28412fb9a068be5169663e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(range(0, len(chunks), 2)):\n",
    "    try:\n",
    "        texts = [chunks[i], chunks[i+1]]\n",
    "    except:\n",
    "        texts = [chunks[i]]\n",
    "    generate_triples(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Miami', 'home venue', 'Cotton Bowl Stadium'),\n",
       " ('Cotton Bowl Stadium', 'occupant', 'Miami'),\n",
       " ('Chinese', 'continent', 'Asia'),\n",
       " ('Old Boys', 'point in time', 'February 15'),\n",
       " ('MLS', 'subsidiary', 'Real Salt Lake'),\n",
       " ('Real Salt Lake', 'league', 'MLS'),\n",
       " ('Tokyo', 'country', 'Japan'),\n",
       " ('Sergio Busquets', 'member of sports team', 'Barcelona'),\n",
       " ('Lionel Messi', 'member of sports team', 'Inter Miami'),\n",
       " ('Jesus Ferreira', 'member of sports team', 'Dallas'),\n",
       " ('Al -Hilal', 'country', 'Saudi Arabia'),\n",
       " ('Vissel Kobe', 'country', 'Japan'),\n",
       " ('Al -Nassr', 'country', 'Saudi Arabia'),\n",
       " ('Luis Suarez', 'member of sports team', 'Barcelona'),\n",
       " ('Maarten Paes', 'member of sports team', 'Dallas'),\n",
       " ('Jesus Ferreira', 'member of sports team', 'US')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_triples = list(set(triples))\n",
    "distinct_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neo4j\n",
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Connect to the Neo4j database\n",
    "URI = os.environ['NEO4J_URI']\n",
    "USER = os.environ['NEO4J_USERNAME']\n",
    "PASSWORD = os.environ['NEO4J_PASSWORD']\n",
    "AUTH = (USER, PASSWORD)\n",
    "\n",
    "driver = GraphDatabase.driver(URI, auth=AUTH)\n",
    "driver.verify_authentication()\n",
    "\n",
    "def create_triplets(tx, triplet) :\n",
    "    tx.run(\"MERGE (s:Node {name: $source})\", source=triplet[0]) \n",
    "    tx.run(\"MERGE (t:Node {name: $target})\", target=triplet[2]) \n",
    "    tx.run(\"MATCH (s:Node {name: $source}), (t:Node {name: $target}) MERGE (s)-[r:Relation {name: $relation} ]->(t)\" ,\n",
    "        source=triplet[0], relation=triplet[1], target=triplet[2])\n",
    "    \n",
    "with driver.session() as session:\n",
    "    for triplet in distinct_triples:\n",
    "        session.execute_write(create_triplets, triplet)\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'datetime.timedelta' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m LLM \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRebel-large\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m file \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mfile_path\n\u001b[1;32m----> 7\u001b[0m processed_time \u001b[38;5;241m=\u001b[39m (end_time \u001b[38;5;241m-\u001b[39m start_time)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhours\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m distinct_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(node \u001b[38;5;28;01mfor\u001b[39;00m triplet \u001b[38;5;129;01min\u001b[39;00m distinct_triples \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m triplet[::\u001b[38;5;241m2\u001b[39m]))\n\u001b[0;32m      9\u001b[0m node_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(distinct_nodes)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'datetime.timedelta' and 'str'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "end_time = datetime.now() \n",
    "\n",
    "LLM = \"Rebel-large\"\n",
    "file = loader.file_path\n",
    "processed_time = (end_time - start_time)\n",
    "distinct_nodes = list(set(node for triplet in distinct_triples for node in triplet[::2]))\n",
    "node_count = len(distinct_nodes)\n",
    "relation_count = len(distinct_triples)\n",
    "\n",
    "data_with_headers = [\n",
    "    {\"LLM\" : LLM, \"File\" : file, \"Processing Time\" : processed_time, \n",
    "     \"Node count\" : node_count, \"Relation count\" : relation_count}\n",
    "]\n",
    "\n",
    "csv_file_path = '../data/llm_comparision.csv'\n",
    "with open(csv_file_path, 'w',newline='') as csv_file:\n",
    "    fieldnames = [\"LLM\",\"File\",\"Processing Time\",\"Node count\",\"Relation count\"]\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data_with_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations :\n",
    "\n",
    "Graph dont have any properties other than name \n",
    "\n",
    "Node count = 23\n",
    "Relation count = 16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
